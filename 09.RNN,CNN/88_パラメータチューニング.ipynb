{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoheiFukuhara/nlp100-2020/blob/main/09.RNN%2CCNN/88_%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSulESOsD6hv"
      },
      "source": [
        "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn-mdO8FOwxQ",
        "outputId": "a44dcbca-21e2-40fc-fb48-2a55042af031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner\n",
        "#!pip show keras-tuner\n",
        "#import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smIMO5cGG3pf",
        "outputId": "50264ad2-9f75-4665-ca05-cb358bbe2c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import IPython\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "import nltk\n",
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zfcn75JDUbC",
        "outputId": "ef7d615e-9599-44a0-9c84-9fb4d3adef5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n",
            "Name: keras-tuner\n",
            "Version: 1.1.0\n",
            "Summary: Hypertuner for Keras\n",
            "Home-page: https://github.com/keras-team/keras-tuner\n",
            "Author: The KerasTuner authors\n",
            "Author-email: kerastuner@google.com\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: kt-legacy, ipython, tensorboard, numpy, scipy, requests, packaging\n",
            "Required-by: \n",
            "---\n",
            "Name: numpy\n",
            "Version: 1.19.5\n",
            "Summary: NumPy is the fundamental package for array computing with Python.\n",
            "Home-page: https://www.numpy.org\n",
            "Author: Travis E. Oliphant et al.\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: yellowbrick, xgboost, xarray, wordcloud, torchvision, torchtext, tifffile, thinc, Theano-PyMC, tensorflow, tensorflow-probability, tensorflow-hub, tensorflow-datasets, tensorboard, tables, statsmodels, spacy, sklearn-pandas, seaborn, scs, scipy, scikit-learn, scikit-image, resampy, qdldl, PyWavelets, python-louvain, pystan, pysndfile, pymc3, pyerfa, pyemd, pyarrow, plotnine, patsy, pandas, osqp, opt-einsum, opencv-python, opencv-contrib-python, numexpr, numba, nibabel, netCDF4, moviepy, mlxtend, mizani, missingno, matplotlib, matplotlib-venn, lightgbm, librosa, keras-tuner, Keras-Preprocessing, kapre, jpeg4py, jaxlib, jax, imgaug, imbalanced-learn, imageio, hyperopt, holoviews, h5py, gym, gensim, folium, fix-yahoo-finance, fbprophet, fastprogress, fastdtw, fastai, fa2, ecos, daft, cvxpy, cupy-cuda111, cufflinks, cmdstanpy, cftime, Bottleneck, bokeh, blis, autograd, atari-py, astropy, arviz, altair, albumentations\n",
            "---\n",
            "Name: gensim\n",
            "Version: 3.6.0\n",
            "Summary: Python framework for fast Vector Space Modelling\n",
            "Home-page: http://radimrehurek.com/gensim\n",
            "Author: Radim Rehurek\n",
            "Author-email: me@radimrehurek.com\n",
            "License: LGPLv2.1\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: smart-open, scipy, numpy, six\n",
            "Required-by: \n",
            "---\n",
            "Name: google\n",
            "Version: 2.0.3\n",
            "Summary: Python bindings to the Google search engine.\n",
            "Home-page: http://breakingcode.wordpress.com/\n",
            "Author: Mario Vilas\n",
            "Author-email: mvilas@gmail.com\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: beautifulsoup4\n",
            "Required-by: \n",
            "---\n",
            "Name: tensorflow\n",
            "Version: 2.7.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: absl-py, wheel, gast, libclang, wrapt, tensorflow-io-gcs-filesystem, protobuf, termcolor, h5py, six, grpcio, typing-extensions, keras, google-pasta, numpy, keras-preprocessing, tensorflow-estimator, astunparse, tensorboard, opt-einsum, flatbuffers\n",
            "Required-by: kapre\n",
            "---\n",
            "Name: nltk\n",
            "Version: 3.2.5\n",
            "Summary: Natural Language Toolkit\n",
            "Home-page: http://nltk.org/\n",
            "Author: Steven Bird\n",
            "Author-email: stevenbird1@gmail.com\n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: six\n",
            "Required-by: textblob\n",
            "---\n",
            "Name: pandas\n",
            "Version: 1.1.5\n",
            "Summary: Powerful data structures for data analysis, time series, and statistics\n",
            "Home-page: https://pandas.pydata.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: pytz, numpy, python-dateutil\n",
            "Required-by: xarray, vega-datasets, statsmodels, sklearn-pandas, seaborn, pymc3, plotnine, pandas-profiling, pandas-gbq, pandas-datareader, mlxtend, mizani, holoviews, gspread-dataframe, google-colab, fix-yahoo-finance, fbprophet, fastai, cufflinks, cmdstanpy, arviz, altair\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip show keras-tuner numpy gensim google tensorflow nltk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJPjoNjhHTc3",
        "outputId": "30dca697-60df-46ec-bb66-d37ed3494128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 30s, sys: 5.41 s, total: 2min 36s\n",
            "Wall time: 2min 47s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "BASE_PATH = '/content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/'\n",
        "max_len = 0\n",
        "vocabulary = []\n",
        "w2v_model = KeyedVectors.load_word2vec_format(BASE_PATH+'07.WordVector/input/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BGk9QJ-sHlPk"
      },
      "outputs": [],
      "source": [
        "def read_dataset(type_):\n",
        "    global max_len\n",
        "    global vocabulary\n",
        "    df = pd.read_table(BASE_PATH+'06.MachineLearning/'+type_+'.feature.txt')\n",
        "    df.info()\n",
        "    sr_title = df['title'].str.split().explode()\n",
        "    max_len_ = df['title'].map(lambda x: len(x.split())).max()\n",
        "    if max_len < max_len_:\n",
        "        max_len = max_len_\n",
        "    if len(vocabulary) == 0:\n",
        "        vocabulary = [k for k, v in nltk.FreqDist(sr_title).items() if v > 1]\n",
        "    else:\n",
        "        vocabulary.extend([k for k, v in nltk.FreqDist(sr_title).items() if v > 1])\n",
        "    y = df['category'].replace({'b':0, 't':1, 'e':2, 'm':3})\n",
        "    return df['title'], tf.keras.utils.to_categorical(y, dtype='int32')  # 4値分類なので訓練・検証・テスト共通でone-hot化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAx17LNLHmtI",
        "outputId": "a782551f-684a-4d3d-eba4-53012409f6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10684 entries, 0 to 10683\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     10684 non-null  object\n",
            " 1   category  10684 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 167.1+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1336 entries, 0 to 1335\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     1336 non-null   object\n",
            " 1   category  1336 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 21.0+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1336 entries, 0 to 1335\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     1336 non-null   object\n",
            " 1   category  1336 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 21.0+ KB\n",
            "vocabulary size before removing duplicates: 11089\n",
            "vocabulary size after removing duplicates: 7802\n",
            "sample vocabulary: ('Traders', 'Jazz', 'style', 'series', 'Leaks', 'hugging', 'Hortons', 'earn', 'role', 'knowle')\n",
            "max length is 18\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = read_dataset('train')\n",
        "X_valid, y_valid = read_dataset('valid')\n",
        "X_test, y_test = read_dataset('test') # あまりこだわらずにテストデータセットも追加\n",
        "\n",
        "# setで重複削除し、タプル形式に設定\n",
        "tup_voc = tuple(set(vocabulary))\n",
        "\n",
        "print(f'vocabulary size before removing duplicates: {len(vocabulary)}')\n",
        "print(f'vocabulary size after removing duplicates: {len(tup_voc)}')\n",
        "print(f'sample vocabulary: {tup_voc[:10]}')\n",
        "print(f'max length is {max_len}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_R8EAy1IPHc",
        "outputId": "3c55e376-aeee-4678-abc4-fcffe278fbef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size is 7804\n"
          ]
        }
      ],
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " output_mode='int',\n",
        " vocabulary=tup_voc,\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "print(f'vocabulary size is {vectorize_layer.vocabulary_size()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eGNTfv6bWlU",
        "outputId": "c25dd5a5-bbfc-4c23-9166-8d116138d74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[UNK]\n",
            "knowle\n",
            "S&P\n",
            "Mccurdy\n",
            "ftse\n",
            "CPU times: user 79.8 ms, sys: 13 ms, total: 92.8 ms\n",
            "Wall time: 144 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "embedding_matrix = np.zeros((vectorize_layer.vocabulary_size(), embedding_dim))\n",
        "for i, word in enumerate(vectorize_layer.get_vocabulary()):\n",
        "    try:\n",
        "        embedding_matrix[i] = w2v_model.get_vector(word)\n",
        "        hits += 1\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "    except:\n",
        "        misses += 1\n",
        "        if misses < 7:  # Show 6 words as example\n",
        "            print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sqjy7bW159Ok"
      },
      "outputs": [],
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    vectorize_layer.vocabulary_size(),\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r_mDjQmeHqi1"
      },
      "outputs": [],
      "source": [
        "def conv_layer(kernel_token, in2emb, filter):\n",
        "    conv = tf.keras.layers.Conv1D(filters=filter, \n",
        "                                 kernel_size=embedding_dim*kernel_token, \n",
        "                                 padding='same',\n",
        "                                 strides=embedding_dim,\n",
        "                                 activation='relu',\n",
        "                                 name='Conv1D_'+str(kernel_token))(in2emb)\n",
        "    conv = tf.keras.layers.MaxPooling1D(pool_size=2, \n",
        "                                        name='MaxPooling1D_'+str(kernel_token))(conv)\n",
        "    conv = tf.keras.layers.Flatten(name='Flatten_'+str(kernel_token))(conv)\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "    hp_gru_units = hp.Int('units', min_value = 32, max_value = 96, step = 32)\n",
        "    hp_gru_dout  = hp.Choice('gru_dropout', values = [0.2, 0.3, 0.4]) \n",
        "    hp_filter = hp.Int('filter', min_value = 2, max_value = 4, step = 1)\n",
        "    hp_dout  = hp.Choice('dropout', values = [0.2, 0.3, 0.4]) \n",
        "\n",
        "    input = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "    in2emb = vectorize_layer(input)\n",
        "    in2emb = embedding_layer(in2emb)\n",
        "    rnn = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.GRU(hp_gru_units,\n",
        "                            dropout=hp_gru_dout,\n",
        "                            return_sequences=True))(in2emb)\n",
        "    rnn = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.GRU(hp_gru_units,\n",
        "                            dropout=hp_gru_dout))(rnn)\n",
        "\n",
        "    in2emb = tf.keras.layers.Reshape((max_len * embedding_dim, 1))(in2emb)\n",
        "\n",
        "    conv3 = conv_layer(2, in2emb, hp_filter)\n",
        "    conv2 = conv_layer(3, in2emb, hp_filter)\n",
        "    conv4 = conv_layer(4, in2emb, hp_filter)\n",
        "\n",
        "    last = tf.keras.layers.concatenate([conv2, conv3, conv4, rnn])\n",
        "    last = tf.keras.layers.Dropout(hp_dout)(last)\n",
        "    last = tf.keras.layers.Dense(4, activation='softmax')(last)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=input, outputs=last)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "UG1-91tDUvLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zBnGitO0UDDL"
      },
      "outputs": [],
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective = 'val_loss', \n",
        "                     max_epochs = 30,\n",
        "                     factor = 3,\n",
        "                     directory = './',\n",
        "                     project_name = 'nlp100-2020')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t66JX43EeQmk"
      },
      "outputs": [],
      "source": [
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHoesMkQOM1p",
        "outputId": "d73e0c91-4043-4351-e280-0a47d2cbf281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 56 Complete [00h 01m 03s]\n",
            "val_loss: 0.7480563521385193\n",
            "\n",
            "Best val_loss So Far: 0.4708493947982788\n",
            "Total elapsed time: 00h 56m 07s\n",
            "\n",
            "Search: Running Trial #57\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units             |64                |96                \n",
            "gru_dropout       |0.3               |0.2               \n",
            "filter            |3                 |4                 \n",
            "dropout           |0.4               |0.2               \n",
            "tuner/epochs      |4                 |30                \n",
            "tuner/initial_e...|0                 |10                \n",
            "tuner/bracket     |2                 |3                 \n",
            "tuner/round       |0                 |3                 \n",
            "\n",
            "Epoch 1/4\n",
            "334/334 [==============================] - 23s 45ms/step - loss: 1.1717 - acc: 0.4955 - val_loss: 1.0956 - val_acc: 0.6946\n",
            "Epoch 2/4\n",
            "334/334 [==============================] - 14s 41ms/step - loss: 1.0309 - acc: 0.6802 - val_loss: 0.9404 - val_acc: 0.7507\n",
            "Epoch 3/4\n",
            "334/334 [==============================] - 14s 40ms/step - loss: 0.8807 - acc: 0.7214 - val_loss: 0.7983 - val_acc: 0.7582\n",
            "Epoch 4/4\n",
            "139/334 [===========>..................] - ETA: 7s - loss: 0.8037 - acc: 0.7336"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "tuner.search(X_train, y_train, epochs=100, \n",
        "             validation_data=(X_valid, y_valid), \n",
        "             callbacks = [ClearTrainingOutput()])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "print(best_hps.values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search_space_summary()\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "7cyyv0JmOBOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVO2x5702S7b"
      },
      "outputs": [],
      "source": [
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE9o6wIyLftu"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=False)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoGchp2OJ_T4"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lraSuLH7KCf9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "88.パラメータチューニング.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgssJJaq7+B8SJa3vOq4FP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}