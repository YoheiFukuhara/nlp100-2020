{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "80.ID番号への変換.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoLP7l1rsOgZgE54271HvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoheiFukuhara/nlp100-2020/blob/main/09.RNN%2CCNN/80_ID%E7%95%AA%E5%8F%B7%E3%81%B8%E3%81%AE%E5%A4%89%E6%8F%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWRz_bU7VfYg"
      },
      "source": [
        "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP22yOjp3lVY",
        "outputId": "22169ed0-1ad6-4ebf-d1cc-f00466d52983"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7rzhXSK365p",
        "outputId": "01119b3f-3288-41fb-e336-5214491430ee"
      },
      "source": [
        "!python --version\n",
        "!pip show google tensorflow nltk pandas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n",
            "Name: google\n",
            "Version: 2.0.3\n",
            "Summary: Python bindings to the Google search engine.\n",
            "Home-page: http://breakingcode.wordpress.com/\n",
            "Author: Mario Vilas\n",
            "Author-email: mvilas@gmail.com\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: beautifulsoup4\n",
            "Required-by: \n",
            "---\n",
            "Name: tensorflow\n",
            "Version: 2.7.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: flatbuffers, h5py, termcolor, numpy, typing-extensions, grpcio, wrapt, absl-py, opt-einsum, google-pasta, wheel, gast, libclang, keras, six, protobuf, tensorflow-io-gcs-filesystem, keras-preprocessing, astunparse, tensorboard, tensorflow-estimator\n",
            "Required-by: kapre\n",
            "---\n",
            "Name: nltk\n",
            "Version: 3.2.5\n",
            "Summary: Natural Language Toolkit\n",
            "Home-page: http://nltk.org/\n",
            "Author: Steven Bird\n",
            "Author-email: stevenbird1@gmail.com\n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: six\n",
            "Required-by: textblob\n",
            "---\n",
            "Name: pandas\n",
            "Version: 1.1.5\n",
            "Summary: Powerful data structures for data analysis, time series, and statistics\n",
            "Home-page: https://pandas.pydata.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: python-dateutil, pytz, numpy\n",
            "Required-by: xarray, vega-datasets, statsmodels, sklearn-pandas, seaborn, pymc3, plotnine, pandas-profiling, pandas-gbq, pandas-datareader, mlxtend, mizani, holoviews, gspread-dataframe, google-colab, fix-yahoo-finance, fbprophet, fastai, cufflinks, cmdstanpy, arviz, altair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSejD9r1g0og"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/06.MachineLearning/'\n",
        "max_len = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_T_xytuDbWC"
      },
      "source": [
        "def get_vocabulary(type_):\n",
        "    global max_len\n",
        "    df = pd.read_table(BASE_PATH+type_+'.feature.txt')\n",
        "    df.info()\n",
        "    sr_title = df['title'].str.split().explode()\n",
        "    max_len_ = df['title'].map(lambda x: len(x.split())).max()\n",
        "    if max_len < max_len_:\n",
        "        max_len = max_len_\n",
        "    return [k for k, v in nltk.FreqDist(sr_title).items() if v > 1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o6mbJdLVRoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa96e06c-3856-4440-d306-4e633ed76c53"
      },
      "source": [
        "vocabulary = get_vocabulary('train')\n",
        "vocabulary.extend(get_vocabulary('valid'))\n",
        "vocabulary.extend(get_vocabulary('test')) # あまりこだわらずにテストデータセットも追加\n",
        "\n",
        "# setで重複削除し、タプル形式に設定\n",
        "tup_voc = tuple(set(vocabulary))\n",
        "\n",
        "print(f'vocabulary size before removing duplicates: {len(vocabulary)}')\n",
        "print(f'vocabulary size after removing duplicates: {len(tup_voc)}')\n",
        "print(f'sample vocabulary: {tup_voc[:10]}')\n",
        "print(f'max length is {max_len}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10684 entries, 0 to 10683\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     10684 non-null  object\n",
            " 1   category  10684 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 167.1+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1336 entries, 0 to 1335\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     1336 non-null   object\n",
            " 1   category  1336 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 21.0+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1336 entries, 0 to 1335\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     1336 non-null   object\n",
            " 1   category  1336 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 21.0+ KB\n",
            "vocabulary size before removing duplicates: 11089\n",
            "vocabulary size after removing duplicates: 7802\n",
            "sample vocabulary: ('Changes', 'rampage', 'Politics', 'Gawker', 'mood', 'Quiznos', 'friendless', 'end', 'Song', 'semi')\n",
            "max length is 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5oTPuyf4nDz",
        "outputId": "d1d9d8a0-e83d-454f-a3cc-b3074ea48921"
      },
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " output_mode='int',\n",
        " vocabulary=tup_voc,\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "print(f'sample vocabulary: {vectorize_layer.get_vocabulary()[:10]}')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "\n",
        "# いまいち仕様を調べていないが、一度modelに追加すると下記が有効になる\n",
        "print(vectorize_layer([[\"bar unknown_word(1)\"]]).numpy)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample vocabulary: ['', '[UNK]', 'Changes', 'rampage', 'Politics', 'Gawker', 'mood', 'Quiznos', 'friendless', 'end']\n",
            "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(1, 18), dtype=int64, numpy=\n",
            "array([[2081,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0]])>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcHAXzIAWUN1"
      },
      "source": [
        "vectorize_layer"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}