{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "70.単語ベクトルの和による特徴量.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1W90KPNheIZxMwzg7fa3fvMLtVZwlTMrx",
      "authorship_tag": "ABX9TyO713dtNZDSjCFZHIlGxK45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoheiFukuhara/nlp100-2020/blob/main/08.%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88/70_%E5%8D%98%E8%AA%9E%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%81%AE%E5%92%8C%E3%81%AB%E3%82%88%E3%82%8B%E7%89%B9%E5%BE%B4%E9%87%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXelmJ5fGR8e"
      },
      "source": [
        "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$X$と正解ラベルを並べた行列（ベクトル）$Y$を作成したい．\n",
        "\n",
        "```math\n",
        "X = \\begin{pmatrix} \n",
        "  \\boldsymbol{x}_1 \\\\ \n",
        "  \\boldsymbol{x}_2 \\\\ \n",
        "  \\dots \\\\ \n",
        "  \\boldsymbol{x}_n \\\\ \n",
        "\\end{pmatrix} \\in \\mathbb{R}^{n \\times d},\n",
        "Y = \\begin{pmatrix} \n",
        "  y_1 \\\\ \n",
        "  y_2 \\\\ \n",
        "  \\dots \\\\ \n",
        "  y_n \\\\ \n",
        "\\end{pmatrix} \\in \\mathbb{N}^{n}\n",
        "```\n",
        "\n",
        "ここで，$n$は学習データの事例数であり，$\\boldsymbol x_i \\in \\mathbb{R}^d$と$y_i \\in \\mathbb N$はそれぞれ，$i \\in \\{1, \\dots, n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb N_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_i \\in \\mathbb N_{<4}$で表現できる．\n",
        "以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
        "\n",
        "$i$番目の事例の特徴ベクトル$\\boldsymbol x_i$は，次式で求める．\n",
        "\n",
        "$$\\boldsymbol x_i = \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\mathrm{emb}(w_{i,t})$$\n",
        "\n",
        "ここで，$i$番目の事例は$T_i$個の（記事見出しの）単語列$(w_{i,1}, w_{i,2}, \\dots, w_{i,T_i})$から構成され，$\\mathrm{emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数は$d$）である．すなわち，$i$番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$\\boldsymbol x_i$である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．$300$次元の単語ベクトルを用いたので，$d=300$である．\n",
        "$i$番目の事例のラベル$y_i$は，次のように定義する．\n",
        "\n",
        "```math\n",
        "y_i = \\begin{cases}\n",
        "0 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「ビジネス」カテゴリの場合}) \\\\\n",
        "1 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「科学技術」カテゴリの場合}) \\\\\n",
        "2 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「エンターテイメント」カテゴリの場合}) \\\\\n",
        "3 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「健康」カテゴリの場合}) \\\\\n",
        "\\end{cases}\n",
        "```\n",
        "\n",
        "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
        "\n",
        "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
        "\n",
        " + 学習データの特徴量行列: $X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$\n",
        " + 学習データのラベルベクトル: $Y_{\\rm train} \\in \\mathbb{N}^{N_t}$\n",
        " + 検証データの特徴量行列: $X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$\n",
        " + 検証データのラベルベクトル: $Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$\n",
        " + 評価データの特徴量行列: $X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$\n",
        " + 評価データのラベルベクトル: $Y_{\\rm test} \\in \\mathbb{N}^{N_e}$\n",
        "\n",
        "なお，$N_t, N_v, N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIzzGb-G-4KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcaadcb3-1840-439f-9574-aed482777c5f"
      },
      "source": [
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JM1s_Jd_G1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e61e2b5-78dd-4229-fa1f-27e2b7c583c0"
      },
      "source": [
        "!python --version\n",
        "!pip show google pandas gensim tensorflow numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n",
            "Name: google\n",
            "Version: 2.0.3\n",
            "Summary: Python bindings to the Google search engine.\n",
            "Home-page: http://breakingcode.wordpress.com/\n",
            "Author: Mario Vilas\n",
            "Author-email: mvilas@gmail.com\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: beautifulsoup4\n",
            "Required-by: \n",
            "---\n",
            "Name: pandas\n",
            "Version: 1.1.5\n",
            "Summary: Powerful data structures for data analysis, time series, and statistics\n",
            "Home-page: https://pandas.pydata.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: numpy, python-dateutil, pytz\n",
            "Required-by: xarray, vega-datasets, statsmodels, sklearn-pandas, seaborn, pymc3, plotnine, pandas-profiling, pandas-gbq, pandas-datareader, mlxtend, mizani, holoviews, gspread-dataframe, google-colab, fix-yahoo-finance, fbprophet, fastai, cufflinks, cmdstanpy, arviz, altair\n",
            "---\n",
            "Name: gensim\n",
            "Version: 3.6.0\n",
            "Summary: Python framework for fast Vector Space Modelling\n",
            "Home-page: http://radimrehurek.com/gensim\n",
            "Author: Radim Rehurek\n",
            "Author-email: me@radimrehurek.com\n",
            "License: LGPLv2.1\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: smart-open, six, scipy, numpy\n",
            "Required-by: \n",
            "---\n",
            "Name: tensorflow\n",
            "Version: 2.6.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: h5py, termcolor, clang, typing-extensions, absl-py, google-pasta, six, numpy, tensorboard, wheel, gast, astunparse, keras-preprocessing, keras, flatbuffers, tensorflow-estimator, grpcio, protobuf, wrapt, opt-einsum\n",
            "Required-by: kapre\n",
            "---\n",
            "Name: numpy\n",
            "Version: 1.19.5\n",
            "Summary: NumPy is the fundamental package for array computing with Python.\n",
            "Home-page: https://www.numpy.org\n",
            "Author: Travis E. Oliphant et al.\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: yellowbrick, xgboost, xarray, wordcloud, torchvision, torchtext, tifffile, thinc, Theano-PyMC, tensorflow, tensorflow-probability, tensorflow-hub, tensorflow-datasets, tensorboard, tables, statsmodels, spacy, sklearn-pandas, seaborn, scs, scipy, scikit-learn, resampy, qdldl, PyWavelets, python-louvain, pystan, pysndfile, pymc3, pyerfa, pyemd, pyarrow, plotnine, patsy, pandas, osqp, opt-einsum, opencv-python, opencv-contrib-python, numexpr, numba, nibabel, netCDF4, moviepy, mlxtend, mizani, missingno, matplotlib, matplotlib-venn, lightgbm, librosa, Keras-Preprocessing, kapre, jpeg4py, jaxlib, jax, imgaug, imbalanced-learn, imageio, hyperopt, holoviews, h5py, gym, gensim, folium, fix-yahoo-finance, fbprophet, fastprogress, fastdtw, fastai, fa2, ecos, daft, cvxpy, cufflinks, cmdstanpy, cftime, Bottleneck, bokeh, blis, autograd, atari-py, astropy, arviz, altair, albumentations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opcce41KUtDo"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/'\n",
        "BASE_PATH08 = BASE_PATH + '08.NeuralNetworks/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxflGWB3yp2X",
        "outputId": "de8d2c0b-bc77-49f1-9d7e-4745225598f8"
      },
      "source": [
        "%%time\n",
        "w2v_model = KeyedVectors.load_word2vec_format(BASE_PATH+'07.WordVector/input/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 15s, sys: 4.58 s, total: 2min 20s\n",
            "Wall time: 2min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXg8PYyX91ja"
      },
      "source": [
        "# 変換テーブル作成: 同じ長さでないとエラーがでるので第2引数の長さ調整(置換元と置換先文字は1:1だから)\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "def vectorize(title):\n",
        "\n",
        "    # 句読点をスペースに置換してsplit\n",
        "    tokens = title.translate(table).split()\n",
        "\n",
        "    # 各Tokenをベクトル化\n",
        "    vectors = [w2v_model[token] for token in tokens if token in w2v_model]\n",
        "    \n",
        "    # Tokenごとの平均ベクトルを算出(DataFrameの要素にNumpy Arrayを格納)\n",
        "    vectors = np.array(sum(vectors)/len(vectors))\n",
        "    return vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2gGX_K-yRz"
      },
      "source": [
        "def make_example(title, category):\n",
        "\n",
        "    # Dictionary形式でfeaturesを格納\n",
        "    features={\n",
        "       'title' : tf.train.Feature(bytes_list=tf.train.BytesList(value=[title])),\n",
        "       'category' : tf.train.Feature(int64_list=tf.train.Int64List(value=[category]))\n",
        "    }\n",
        "\n",
        "    # ExampleとFeaturesでまとめて返す\n",
        "    return tf.train.Example(features=tf.train.Features(feature=features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7TFbgFCE3nD"
      },
      "source": [
        "def process_data(type_):\n",
        "    df = pd.read_table(BASE_PATH08+'input/'+type_+'.txt')\n",
        "\n",
        "    # タイトル: Token化しベクトル化した要素を1列にしたSeries作成\n",
        "    titles = df['title'].map(vectorize).explode().astype('float32')\n",
        "\n",
        "    # タイトル: 配列をNumpy形式にしてshape変更\n",
        "    titles = titles.values.reshape(len(df), 300)\n",
        "\n",
        "    # ラベル: 文字を数字に変更\n",
        "    categories = df['category'].replace({'b':0, 't':1, 'e':2, 'm':3})\n",
        "\n",
        "    # ファイル書込\n",
        "    with tf.io.TFRecordWriter(BASE_PATH08+type_+'.tfrecord') as writer:\n",
        "        \n",
        "        # １行ずつ取り出してExample形式にして書込\n",
        "        for i, (title, category) in enumerate(zip(titles, categories)):\n",
        "            ex = make_example(title.tobytes(), category)\n",
        "            writer.write(ex.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rovx-HnHE6gK",
        "outputId": "1d6c115c-e71c-4f40-e05a-d93dbe06f629"
      },
      "source": [
        "%%time\n",
        "process_data('train')\n",
        "process_data('valid')\n",
        "process_data('test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.9 s, sys: 176 ms, total: 2.08 s\n",
            "Wall time: 3.83 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9tPyk4PSl8I",
        "outputId": "197abc3b-39c1-4ac6-dd86-ef7219ce440b"
      },
      "source": [
        "!find /content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/08.NeuralNetworks -name '*.tfrecord' -ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       82   1640 -rw-------   1 root     root      1679352 Sep 25 04:44 /content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/08.NeuralNetworks/test.tfrecord\n",
            "       77   1640 -rw-------   1 root     root      1679352 Sep 25 04:44 /content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/08.NeuralNetworks/valid.tfrecord\n",
            "       72  13116 -rw-------   1 root     root     13429788 Sep 25 04:44 /content/drive/MyDrive/ColabNotebooks/ML/NLP100_2020/08.NeuralNetworks/train.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wE1-V_-bM2x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}